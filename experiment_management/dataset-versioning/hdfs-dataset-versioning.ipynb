{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking HDFS dataset versions using Verta\n",
    "\n",
    "Verta's experiment management system enables data scientists to track dataset versions stored on HDFS.\n",
    "\n",
    "This notebook shows how to use Verta's experiment management system to track dataset versions. See Verta [documentation](https://verta.readthedocs.io/en/master/_autogen/verta.dataset.HDFSPath.html) for full details on Verta's dataset versioning capabilities.\n",
    "\n",
    "Updated for Verta version: 0.18.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "\n",
    "# !pip install pyspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Verta import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart your notebook if prompted on Colab\n",
    "try:\n",
    "    import verta\n",
    "except ImportError:\n",
    "    !pip install verta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['VERTA_EMAIL'] = \n",
    "# os.environ['VERTA_DEV_KEY'] = \n",
    "# os.environ['VERTA_HOST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "import os\n",
    "\n",
    "client = Client(os.environ['VERTA_HOST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.dataset import HDFSPath\n",
    "\n",
    "# NOTE: we need HDFS running on a host and port\n",
    "hdfs = \"hdfs://HOST:PORT\"\n",
    "\n",
    "dataset = client.set_dataset(name=\"Census Income HDFS\")\n",
    "blob = HDFSPath.with_spark(sc, \"{}/data/census/*\".format(hdfs))\n",
    "dataset_version = dataset.create_version(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset, dataset_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = sc.textFile(\"{}/data/census/census-train.csv\".format(hdfs)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.external.six import StringIO\n",
    "\n",
    "df_train = pd.read_csv(StringIO('\\n'.join(csv)))\n",
    "X_train = df_train.iloc[:,:-1]\n",
    "y_train = df_train.iloc[:, -1]\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create object to track experiment run\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "# log training data\n",
    "run.log_dataset_version(\"train\", dataset_version)\n",
    "\n",
    "# ---------------------- other tracking below ------------------------\n",
    "\n",
    "# create validation split\n",
    "(X_val_train, X_val_test,\n",
    " Y_val_train, Y_val_test) = model_selection.train_test_split(X_train, Y_train,\n",
    "                                                             test_size=0.2,\n",
    "                                                             shuffle=True)\n",
    "# log hyperparameters\n",
    "hyperparams = {\"C\" : 10}\n",
    "run.log_hyperparameters(hyperparams)\n",
    "print(hyperparams, end=' ')\n",
    "\n",
    "# create and train model\n",
    "model = linear_model.LogisticRegression(**hyperparams)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# calculate and log validation accuracy\n",
    "val_acc = model.score(X_val_test, Y_val_test)\n",
    "run.log_metric(\"val_acc\", val_acc)\n",
    "print(\"Validation accuracy: {:.4f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset version info\n",
    "run.get_dataset_version(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
