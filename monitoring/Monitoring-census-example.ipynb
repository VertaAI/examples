{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor Classification Models in Verta Automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verta can automatically monitor any model deployed via the Verta deployment system. \n",
    "\n",
    "This notebook shows how a classification model on tabular data can be monitored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0.1 Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import multiprocessing.dummy\n",
    "\n",
    "try:\n",
    "    import cloudpickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "    from sklearn import linear_model\n",
    "    import verta\n",
    "    from verta import Client, environment\n",
    "    from verta.dataset import Path\n",
    "    from verta.dataset.entities import Dataset\n",
    "    from verta.environment import Python\n",
    "    from verta.registry import VertaModelBase, verify_io\n",
    "    from verta.registry.entities import RegisteredModel, RegisteredModelVersion\n",
    "    from verta.tracking.entities import ExperimentRun\n",
    "    from verta.utils import ModelAPI\n",
    "    import wget\n",
    "except ImportError:\n",
    "    !pip install numpy pandas sklearn verta wget cloudpickle\n",
    "    import cloudpickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "    from sklearn import linear_model\n",
    "    import verta\n",
    "    from verta import Client, environment\n",
    "    from verta.dataset import Path\n",
    "    from verta.dataset.entities import Dataset\n",
    "    from verta.environment import Python\n",
    "    from verta.registry import VertaModelBase, verify_io\n",
    "    from verta.registry.entities import RegisteredModel, RegisteredModelVersion\n",
    "    from verta.tracking.entities import ExperimentRun\n",
    "    from verta.utils import ModelAPI\n",
    "    import wget\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Verta Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local env vars or uncomment and fill out the lines below:\n",
    "# os.environ['VERTA_EMAIL'] = ''\n",
    "# os.environ['VERTA_DEV_KEY'] = ''\n",
    "# os.environ['VERTA_HOST'] = ''\n",
    "\n",
    "client: Client = Client()\n",
    "\n",
    "# Naming convention to be used for this example\n",
    "NAME: str = 'census-clf-with-monitoring-example-eric-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download Example Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\n",
    "    os.path.dirname('data/examples/'),\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "# Fetch example data from our public S3 bucket.\n",
    "train_data_url = \"http://s3.amazonaws.com/verta-starter/census-train.csv\"\n",
    "train_data_filename = wget.detect_filename(train_data_url)\n",
    "if not os.path.isfile(train_data_filename):\n",
    "    wget.download(\n",
    "        train_data_url,\n",
    "        out='./data/examples/',\n",
    "        )\n",
    "\n",
    "df_train: pd.DataFrame = pd.read_csv(f\"./data/examples/{train_data_filename}\")\n",
    "\n",
    "test_data_url = \"http://s3.amazonaws.com/verta-starter/census-test.csv\"\n",
    "test_data_filename = wget.detect_filename(test_data_url)\n",
    "if not os.path.isfile(test_data_filename):\n",
    "    wget.download(\n",
    "        test_data_url,\n",
    "        out='./data/examples/',\n",
    "        )\n",
    "\n",
    "df_test: pd.DataFrame = pd.read_csv(f\"./data/examples/{test_data_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = dict()\n",
    "# Note: accurate data types are required for monitoring to use the correct metrics\n",
    "\n",
    "for column in df_train.columns:\n",
    "    if column in [\"age\",\"capital-gain\",\"capital-loss\",\"hours-per-week\"]:\n",
    "        dtypes[column] = int\n",
    "    else:\n",
    "        dtypes[column] = bool # turn int to bool to capture binary nature of the remaining columns\n",
    "\n",
    "df_train = df_train.astype(dtypes)\n",
    "\n",
    "X_train = df_train.iloc[:, :-1]\n",
    "Y_train = df_train.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(C=1e-5, solver='lbfgs', max_iter=100)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Create and Register a Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Define Model\n",
    "Create a very basic example model with the minimum required functions (`init` and `predict`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CensusIncomeClassifier(VertaModelBase):\n",
    "    def __init__(self, artifacts):\n",
    "        self.model = cloudpickle.load(open(artifacts[\"serialized_model\"], \"rb\"))\n",
    "\n",
    "    @verify_io\n",
    "    def predict(self, batch_input):\n",
    "        # Model produces True/False that is being turned into 0/1\n",
    "        prediction = map(lambda p : 1 if p else 0, self.model.predict(batch_input).tolist())\n",
    "        confidence_percentage = self.model.predict_proba(batch_input).max(axis=1).tolist()\n",
    "        return list(zip(prediction, confidence_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"./data/model.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(model, f)\n",
    "artifacts_dict = {\"serialized_model\" : \"./data/model.pkl\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Register the Model in Verta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "registered_model: RegisteredModel = client.get_or_create_registered_model(name=NAME)\n",
    "\n",
    "# Note: As of Verta Release 2022.04, confidence scores are recommended for classification models\n",
    "# in order to accurate compute ROC and PR curves.\n",
    "\n",
    "# Code below adds a dummy confidence column to the output to register this column in the system\n",
    "# Note the naming convention of \".confidence\" to identify the column as the confidence score\n",
    "Y_train_with_confidence: pd.DataFrame = pd.merge(\n",
    "    Y_train.to_frame(),\n",
    "    0. * Y_train.to_frame().rename(columns={Y_train.name: Y_train.name + \".confidence\"}),\n",
    "    left_index=True,\n",
    "    right_index=True)\n",
    "\n",
    "model_version: RegisteredModelVersion = registered_model.create_standard_model(\n",
    "    name = \"v1\",\n",
    "    model_cls = CensusIncomeClassifier,\n",
    "    model_api = ModelAPI(X_train, Y_train_with_confidence),\n",
    "    environment = Python(requirements=[\"scikit-learn\"]),\n",
    "    artifacts = artifacts_dict\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Log Reference Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload reference data as a dataset version. This is your training dataset and will help facilitate downstream drift monitoring against this reference set. You do not need to upload your entire training set, but a statistically significant representation that mirrors your training/reference data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch downloaded training data\n",
    "dataset: Dataset = client.get_or_create_dataset(NAME)\n",
    "content: Path = Path([f\"./data/examples/{train_data_filename}\"], enable_mdb_versioning=True)\n",
    "dataset_version = dataset.create_version(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_version.log_dataset_version(key='reference', dataset_version=dataset_version)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy Model and Start Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model is deployed, along with a live endpoint, a new entry is created in the model monitoring tab with the endpoint name, default monitoring dashboards are created and all the features, predictions and drift alerts are configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = client.get_or_create_endpoint(NAME)\n",
    "endpoint.update(model_version, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Make Predictions and Log Ground-truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next snippets of code simulate the real-world where:\n",
    "\n",
    "- caller sends input data for prediction\n",
    "- model makes a prediction and assigs a unique UUID for the prediction\n",
    "- ground truth is then registered with the system using the above UUID\n",
    "\n",
    "Once the data has been sent to the system, you can navigate to the webapp to view dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_predictions(endpoint, deployed_model, input_data, ground_truth, col_name, ground_truth_delay): \n",
    "    # ground_truth_delay is delay in seconds between prediction & GT becoming available\n",
    "    import time\n",
    "    \n",
    "    ids = []\n",
    "    for i, row in input_data.iterrows():\n",
    "        _id, _ = deployed_model.predict_with_id([row.tolist()])\n",
    "        ids.append(_id)\n",
    "\n",
    "    time.sleep(ground_truth_delay)\n",
    "    id_and_gt = zip(ids, ground_truth)\n",
    "    for t in id_and_gt:\n",
    "        endpoint.log_ground_truth(t[0], [t[1]], col_name) # id, gt, prediction_col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = endpoint.get_deployed_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_predictions(endpoint, deployed_model, df_test.iloc[:100, :-1], df_test.iloc[:100, -1], \">50k\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Introduce Drift into the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_age_input_features = df_test.copy()\n",
    "skewed_age_input_features['age'] = skewed_age_input_features['age'] + 20\n",
    "\n",
    "simulate_predictions(endpoint, deployed_model, \n",
    "                     skewed_age_input_features.iloc[600:700, :-1], \n",
    "                     df_test.iloc[600:700, -1], \">50k\", 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
