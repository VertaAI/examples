{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056834,
     "end_time": "2020-11-28T23:01:34.209435",
     "exception": false,
     "start_time": "2020-11-28T23:01:34.152601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Monitoring ML datasets on Verta\n",
    "\n",
    "Verta provides a extensible [model monitoring framework](https://docs.verta.ai/verta/monitoring) that allows the user to fully define and configure what data to monitor and how to monitor it.\n",
    "\n",
    "This notebook shows an example of how Verta model monitoring can be used to define custom monitors on datasets used to build ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Verta import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart your notebook if prompted on Colab\n",
    "try:\n",
    "    import verta\n",
    "except ImportError:\n",
    "    !pip install verta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['VERTA_EMAIL'] = \n",
    "# os.environ['VERTA_DEV_KEY'] = \n",
    "# os.environ['VERTA_HOST']\n",
    "\n",
    "from verta import Client\n",
    "client = Client(os.environ['VERTA_HOST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "def download_data_if_missing(filename):\n",
    "    url = \"https://verta-demo.s3-us-west-2.amazonaws.com/{}\".format(filename)\n",
    "    local_data_file = \"../data/{}\".format(filename)\n",
    "    if not os.path.isfile(local_data_file):\n",
    "        wget.download(url, out=local_data_file)\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df.drop(['id'], axis=1)\n",
    "\n",
    "    df.loc[df['Gender'] == 'Male', 'Gender'] = 1\n",
    "    df.loc[df['Gender'] == 'Female', 'Gender'] = 0\n",
    "\n",
    "    df.loc[df['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\n",
    "    df.loc[df['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\n",
    "    df.loc[df['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\n",
    "\n",
    "    df.loc[df['Vehicle_Damage'] == 'Yes', 'Vehicle_Damage'] = 1\n",
    "    df.loc[df['Vehicle_Damage'] == 'No', 'Vehicle_Damage'] = 0\n",
    "    return df\n",
    "\n",
    "filename = \"xselltrain.csv\"\n",
    "download_data_if_missing(filename)\n",
    "train = pd.read_csv(\"../data/{}\".format(filename))\n",
    "train = clean_data(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define monitored entities\n",
    "\n",
    "In Verta Model Monitoring, a Monitored entity (ME) encapsulates the thing being monitored, e.g., a model, a pipeline, and acts as a context within which data summaries are produced and analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = client.monitoring.get_or_create_monitored_entity(\"insurance-cross-sell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define data summaries and summary samples\n",
    "\n",
    "For a specific ME, there are particular aspects of the data that we wish to monitor, e.g., for a model, we may want to monitor the inputs and outputs; for a dataset, we may want to monitor values in each column of the dataset.\n",
    "\n",
    "So the next step is to define the data _summaries_ we wish to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries and Summary Samples\n",
    "\n",
    "Users who are monitoring a deployed model, a data pipeline, or a model training process may be interested in many kinds of summary data including any number of summary statistics or data distribution summaries such as histograms. Within Verta's tools for data monitoring, a _summary_ defines a class of data which the user is interested in, for example a mean squared error or a histogram of data table column values. A _summary sample_ is an instance of that summary, which might be logged from a training epoch or a batch of inputs and outputs for a deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose in this case, we would like to monitor data summaries for a specific set of columns in our data, then\n",
    "# here's how we could define a generic function to define those summaries\n",
    "\n",
    "continuous_columns = [\"Age\", \"Annual_Premium\", \"Vintage\"]\n",
    "discrete_columns = [\"Gender\", \"Driving_License\", \"Previously_Insured\", \"Vehicle_Damage\", \"Vehicle_Age\"]\n",
    "all_columns = continuous_columns + discrete_columns\n",
    "\n",
    "from verta.data_types import (\n",
    "    DiscreteHistogram,\n",
    "    FloatHistogram,\n",
    "    NumericValue,\n",
    ")\n",
    "\n",
    "from verta.monitoring.profiler import (\n",
    "    MissingValuesProfiler,\n",
    "    BinaryHistogramProfiler,\n",
    "    ContinuousHistogramProfiler,\n",
    ")\n",
    "\n",
    "def profile(data, labels, start_time, end_time, monitored_entity):        \n",
    "    for col in continuous_columns:\n",
    "        summary_name = col + \"-Histogram\"\n",
    "        summary = client.monitoring.summaries.get_or_create(summary_name, FloatHistogram, monitored_entity)\n",
    "        summary_samples = ContinuousHistogramProfiler(columns=[col], bins=[x*3000 for x in range(20)]).profile(data)\n",
    "\n",
    "        for _, histogram in summary_samples.items():  \n",
    "            summary.log_sample(histogram, labels, start_time, end_time)\n",
    "        \n",
    "    for col in discrete_columns:    \n",
    "        summary_name = col + \"-Histogram\"\n",
    "        summary = client.monitoring.summaries.get_or_create(summary_name, DiscreteHistogram, monitored_entity)\n",
    "        summary_samples = BinaryHistogramProfiler(columns=[col]).profile(data)\n",
    "\n",
    "        for _, histogram in summary_samples.items():  \n",
    "            summary.log_sample(histogram, labels, start_time, end_time)\n",
    "\n",
    "    for col in all_columns:\n",
    "        missing_summary_name = col + \"-Missing\"\n",
    "        missing_summary = client.monitoring.summaries.get_or_create(missing_summary_name, DiscreteHistogram, monitored_entity)                \n",
    "        summary_samples = MissingValuesProfiler(columns=[col]).profile(data)\n",
    "\n",
    "        for _, missing_counts in summary_samples.items():  \n",
    "            missing_summary.log_sample(missing_counts, labels, start_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define alerts\n",
    "\n",
    "In many ways, monitors and summaries are just a way to get to our objective; know when unexpected things happen in the system. So next, we define alerts to notify us when somethin unexpected happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.monitoring.notification_channel import SlackNotificationChannel\n",
    "from verta.monitoring.alert import ReferenceAlerter\n",
    "from verta.monitoring.comparison import GreaterThan\n",
    "from verta.monitoring.summaries.queries import SummaryQuery\n",
    "from verta.monitoring.summaries.queries import SummarySampleQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "today = datetime.now(timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = None\n",
    "\n",
    "# supply a Slack notification channel, if available\n",
    "# channel = monitoring.notification_channels.get_or_create(\n",
    "#     \"Demo Monitoring Alerts\",\n",
    "#     SlackNotificationChannel(webhook_url)\n",
    "# )\n",
    "\n",
    "def set_alerts(monitored_entity):\n",
    "    summaries = client.monitoring.summaries.find(SummaryQuery(\n",
    "            monitored_entities=[monitored_entity.id],\n",
    "        ))\n",
    "    for summary in summaries:\n",
    "        threshold = 0.2\n",
    "        ref_sample = summary.find_samples(SummarySampleQuery(labels={\"source\":\"reference\"}))[0]\n",
    "        alerter = ReferenceAlerter(\n",
    "            GreaterThan(threshold),\n",
    "            ref_sample,\n",
    "        )\n",
    "        alert = summary.alerts.create(\n",
    "            summary.name + \"- ReferenceDeviation GT {}\".format(threshold),\n",
    "            alerter,\n",
    "            # notification_channels=[channel], # uncomment if channel is supplied\n",
    "            starting_from=today-timedelta(hours=30), # pick a suitable time from which the alerter should be enabled\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Incorporate profiling functions into your workflow\n",
    "A typical data monitoring workflow works as follows: \n",
    "1. Log reference summaries (e.g., for training data, at training time)\n",
    "2. Log live/new summaries (e.g., when a daily job is re-run or when a model makes predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Log reference summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile(train, {\"source\" : \"reference\"}, today - timedelta(hours=30), today - timedelta(hours=30), me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: as defined above, our alerts need a reference sample to work correctly, so alerts must be set after logging\n",
    "# reference summary samples\n",
    "set_alerts(me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Log live/new summaries\n",
    "\n",
    "Suppose in this case that we have a new dataset and we want to make sure that the new data matches the reference one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = \"xselltest.csv\"\n",
    "download_data_if_missing(test_filename)\n",
    "test = pd.read_csv(\"../data/{}\".format(test_filename))\n",
    "test = clean_data(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some interesting subsets of dataa\n",
    "import numpy as np\n",
    "\n",
    "test_low_premium_customers = test[test.Annual_Premium < 5000]\n",
    "test_high_premium_customers = test[test.Annual_Premium >= 5000]\n",
    "hp_splits = np.array_split(test_high_premium_customers, 25)\n",
    "lp_splits = np.array_split(test_low_premium_customers, 5)\n",
    "test_prev_insured_customers = test[test.Previously_Insured == 1]\n",
    "pi_splits = np.array_split(test_prev_insured_customers, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's some data that looks like the reference and should not produce alerts\n",
    "for idx in range(len(hp_splits[:5])):\n",
    "    profile(\n",
    "        hp_splits[idx], \n",
    "        {\"source\": \"high_premium\"}, \n",
    "        today-timedelta(hours=30 - idx - 1), \n",
    "        today-timedelta(hours=30 - idx - 1),\n",
    "        me\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's some data that does not look like the reference and should produce alerts\n",
    "for idx in range(len(lp_splits)):\n",
    "    profile(\n",
    "        lp_splits[idx], \n",
    "        {\"source\": \"low_premium\"}, \n",
    "        today-timedelta(hours=5 - idx - 1), \n",
    "        today-timedelta(hours=5 - idx - 1),\n",
    "        me\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's some data that does not look like the reference and should produce alerts\n",
    "for idx in range(len(pi_splits[:5])):\n",
    "    profile(\n",
    "        pi_splits[idx], \n",
    "        {\"source\": \"prev_insured\"}, \n",
    "        today-timedelta(hours=5 -idx - 1), \n",
    "        today-timedelta(hours=5 -idx-1),\n",
    "        me\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
