{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Regression Model - Integrated Registry, Endpoint, MonitoringÂ¶\n",
    "\n",
    "\n",
    "Verta can automatically monitor any model deployed via the Verta deployment system. \n",
    "\n",
    "This notebook shows how a regression model on tabular data can be monitored in Verta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart your notebook if prompted on Colab\n",
    "#!python -m pip install verta\n",
    "#!python -m pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Verta import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got VERTA_EMAIL from environment\n",
      "got VERTA_DEV_KEY from environment\n",
      "connection successfully established\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure credentials are set up, if not, use below\n",
    "# os.environ['VERTA_EMAIL'] = ''\n",
    "# os.environ['VERTA_DEV_KEY'] = ''\n",
    "# os.environ['VERTA_HOST'] = ''\n",
    "\n",
    "os.environ['VERTA_EMAIL'] = 'meeta@verta.ai'\n",
    "os.environ['VERTA_DEV_KEY'] = '54b3faff-9059-49fe-8e2b-565c1e4c78e8'\n",
    "os.environ['VERTA_HOST'] = 'staging.dev.verta.ai'\n",
    "\n",
    "from verta import Client\n",
    "client = Client(os.environ['VERTA_HOST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import wget\n",
    "\n",
    "melbourne_file_path = \"melb-data.csv\"\n",
    "if not os.path.isfile(melbourne_file_path):\n",
    "    wget.download(\"http://s3.amazonaws.com/verta-starter/\" + melbourne_file_path)\n",
    "\n",
    "melbourne_data = pd.read_csv(melbourne_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with missing values\n",
    "melbourne_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "# Choose target and features\n",
    "y = melbourne_data.Price\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'Distance',\n",
    "                        'YearBuilt', 'Car', 'Propertycount']\n",
    "X = melbourne_data[melbourne_features]\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "# The split is based on a random number generator. Supplying a numeric value to\n",
    "# the random_state argument guarantees we get the same split every time we\n",
    "# run this script.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize prices to base unit of $1M\n",
    "y_train = y_train / 1e6\n",
    "y_test = y_test / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Train/test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2233983346137293\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(X_train, y_train)\n",
    "melb_preds = forest_model.predict(X_test)\n",
    "print(mean_absolute_error(y_test, melb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register Model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(forest_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.registry import VertaModelBase, verify_io\n",
    "\n",
    "class HousingPriceRegressor(VertaModelBase):\n",
    "    def __init__(self, artifacts):\n",
    "        self.model = cloudpickle.load(open(artifacts[\"serialized_model\"], \"rb\"))\n",
    "        \n",
    "    @verify_io\n",
    "    def predict(self, batch_input):\n",
    "        return self.model.predict(batch_input).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9889150000000002,\n",
       " 0.5177234399999994,\n",
       " 0.8662029999999997,\n",
       " 1.5177249999999995,\n",
       " 0.732605]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifacts_dict = {\"serialized_model\" : \"model.pkl\"}\n",
    "clf = HousingPriceRegressor(artifacts_dict)\n",
    "clf.predict(X_test.values.tolist()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new RegisteredModel: melbourne-housing-data in workspace: Test-Mar7\n"
     ]
    }
   ],
   "source": [
    "registered_model = client.get_or_create_registered_model(\n",
    "    name=\"melbourne-housing-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new ModelVersion: v1\n",
      "uploading serialized_model to Registry\n",
      "uploading part 1\r"
     ]
    }
   ],
   "source": [
    "from verta.environment import Python\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "model_version = registered_model.create_standard_model(\n",
    "    model_cls=HousingPriceRegressor,\n",
    "    environment=Python(requirements=[\"scikit-learn\"]),\n",
    "    artifacts=artifacts_dict,\n",
    "    name=\"v1\",\n",
    "    model_api=ModelAPI(X_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model type data so the system can compute appropriate model performance metrics\n",
    "\n",
    "model_version.add_attributes({\n",
    "    'model_type': \"regression\",\n",
    " })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Log reference data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload your reference data as an artifact in your Regstered Model Version. This is your training dataset and will help facilitate downstream drift monitoring against this reference set. You dont need to upload your entire training set, but a statistically significant representation that mirrors your training/reference data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version.log_reference_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy model to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endpoint = client.get_or_create_endpoint(\"melbourne-housing-data\")\n",
    "endpoint.update(model_version, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run predictions and log groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_predictions(endpoint, deployed_model, input_data, ground_truth, col_name, ground_truth_delay): \n",
    "    # ground_truth_delay is delay in seconds between prediction & GT becoming available\n",
    "    import time\n",
    "    \n",
    "    ids = []\n",
    "    for i, row in X_train.iterrows():\n",
    "        _id, _ = deployed_model.predict_with_id([row.tolist()])\n",
    "        ids.append(_id)\n",
    "\n",
    "    time.sleep(ground_truth_delay)\n",
    "    \n",
    "    id_and_gt = zip(ids, ground_truth)\n",
    "    \n",
    "    for t in id_and_gt:\n",
    "        endpoint.log_ground_truth(t[0], [t[1]], col_name) # id, gt, prediction_col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = endpoint.get_deployed_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (deployed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_predictions(endpoint, deployed_model, X_test.values.tolist()[0:2], y_test.values.tolist()[0:2], \"Price\", 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
