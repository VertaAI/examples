{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 17:50:31.838550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-29 17:50:32.830194: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-29 17:50:32.830268: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-29 17:50:32.830279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import configparser\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from verta import Client\n",
    "from verta.endpoint.autoscaling import Autoscaling\n",
    "from verta.endpoint.autoscaling.metrics import CpuUtilizationTarget, MemoryUtilizationTarget, RequestsPerWorkerTarget\n",
    "from verta.endpoint.resources import Resources\n",
    "from verta.endpoint.update import DirectUpdateStrategy\n",
    "from verta.environment import Python\n",
    "from verta.registry import VertaModelBase, verify_io\n",
    "from verta.utils import ModelAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectObject(VertaModelBase):\n",
    "    def __init__(self, artifacts=None):\n",
    "        module_handle = 'https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1'\n",
    "        self.detector = hub.load(module_handle).signatures['default']\n",
    "    \n",
    "    def handle_img(self, img, width=640, height=480):\n",
    "        _, path = tempfile.mkstemp(suffix='.jpg')\n",
    "        img_str = json.loads(img)\n",
    "        img_bytes = img_str.encode('utf-8')\n",
    "        img_bytes = io.BytesIO(base64.b64decode(img_bytes))\n",
    "        img_arr = np.array(Image.open(img_bytes), dtype=np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        img = ImageOps.grayscale(img)\n",
    "        img.thumbnail((width, height), Image.Resampling.LANCZOS)\n",
    "        img.save(path, format = 'JPEG', quality = 90)\n",
    "        \n",
    "        print(f\"Image downloaded to {path}.\")\n",
    "        return path\n",
    "\n",
    "    def load_img(self, path):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        \n",
    "        return img\n",
    "\n",
    "    def filter_results(self, file, response, entity='Car', min_score=.2):\n",
    "        unused_keys = ['detection_class_labels', 'detection_class_names']\n",
    "        response = {key: value.numpy().tolist() for key, value in response.items()}\n",
    "        response = {key: val for key, val in response.items() if key not in unused_keys}\n",
    "        response['detection_class_entities'] = [v.decode() for v in response['detection_class_entities']]\n",
    "\n",
    "        entities = response['detection_class_entities']\n",
    "        scores = response['detection_scores']\n",
    "        bboxes = response['detection_boxes']\n",
    "        result = {}\n",
    "\n",
    "        for i in range(len(entities)):\n",
    "            if entities[i] == entity and scores[i] >= min_score:\n",
    "                ymin, xmin, ymax, xmax = bboxes[i]\n",
    "                result = {\n",
    "                    'file': file,\n",
    "                    'has_car': True,\n",
    "                    'score': scores[i],\n",
    "                    'bboxes': {'ymin': ymin, 'xmin': xmin, 'ymax': ymax, 'xmax': xmax}\n",
    "                }\n",
    "                break\n",
    "\n",
    "        if len(result) == 0:\n",
    "            result = {\n",
    "                'file': file,\n",
    "                'has_car': False,\n",
    "                'score': 0,\n",
    "                'bboxes': {'ymin': 0, 'xmin': 0, 'ymax': 0, 'xmax': 0}\n",
    "            }\n",
    "        \n",
    "        print(f\"Found {result['has_car']} object(s).\")\n",
    "        return result\n",
    "\n",
    "    def run_detector(self, file, image_path):\n",
    "        img = self.load_img(image_path)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "        \n",
    "        response = self.detector(img)\n",
    "        result = self.filter_results(file, response)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def detect_objects(self, file, img_arr):\n",
    "        start_time = time.time()\n",
    "        image_path = self.handle_img(img_arr)\n",
    "        result = self.run_detector(file, image_path)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"Inference time: {end_time - start_time}.\")\n",
    "        return result\n",
    "\n",
    "    @verify_io\n",
    "    def predict(self, data):\n",
    "        file, img_arr = data\n",
    "        result = self.detect_objects(file, img_arr)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "VERTA_HOST = config['APP']['VERTA_HOST']\n",
    "PROJECT_NAME = config['APP']['PROJECT_NAME']\n",
    "MODEL_NAME = config['APP']['MODEL_NAME']\n",
    "ENDPOINT_NAME = config['APP']['ENDPOINT_NAME']\n",
    "\n",
    "os.environ['VERTA_EMAIL'] = config['APP']['VERTA_EMAIL']\n",
    "os.environ['VERTA_DEV_KEY'] = config['APP']['VERTA_DEV_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got VERTA_EMAIL from environment\n",
      "got VERTA_DEV_KEY from environment\n",
      "connection successfully established\n",
      "got existing Project: Object Detection V2\n",
      "got existing RegisteredModel: Object Detection V2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfascina/Projetos/VertaAI/examples/deployment/computer-vision/base64/.env/lib/python3.10/site-packages/verta/_internal_utils/_utils.py:1457: UserWarning: Registered Model with name Object Detection V2 already exists; cannot set `desc`, `labels`, `public_within_org`, or `visibility`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = Client(VERTA_HOST)\n",
    "project = client.set_project(PROJECT_NAME)\n",
    "registered_model = client.get_or_create_registered_model(\n",
    "    name = PROJECT_NAME, \n",
    "    labels = ['object-detection']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"file_name\": \"\",\n",
    "    \"image_str\": \"\"\n",
    "}\n",
    "output = {\n",
    "    \"file_name\": \"\",\n",
    "    \"has_car\": False,\n",
    "    \"score\": 0,\n",
    "    \"ymin\": 0,\n",
    "    \"xmin\": 0,\n",
    "    \"ymax\": 0,\n",
    "    \"xmax\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new ModelVersion: base64\n",
      "uploading model to Registry\n",
      "uploading part 1\n",
      "upload complete\n",
      "uploading model_api.json to Registry\n",
      "uploading part 1\n",
      "upload complete\n",
      "uploading custom_modules to Registry\n",
      "uploading part 1\n",
      "upload complete\n"
     ]
    }
   ],
   "source": [
    "model = registered_model.create_standard_model(\n",
    "    model_cls = DetectObject,\n",
    "    environment = Python(requirements = ['tensorflow', 'tensorflow_hub', 'matplotlib']),\n",
    "    model_api = ModelAPI([input], [output]),\n",
    "    name = MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscaling = Autoscaling(min_replicas = 1, max_replicas = 20, min_scale = 0.1, max_scale = 10)\n",
    "autoscaling.add_metric(CpuUtilizationTarget(0.6))\n",
    "autoscaling.add_metric(MemoryUtilizationTarget(0.7))\n",
    "autoscaling.add_metric(RequestsPerWorkerTarget(1))\n",
    "resources = Resources(cpu = 2., memory = '12Gi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for update................................................................\n"
     ]
    }
   ],
   "source": [
    "endpoint = client.get_or_create_endpoint(ENDPOINT_NAME)\n",
    "status = endpoint.update(\n",
    "    model, \n",
    "    strategy = DirectUpdateStrategy(),\n",
    "    autoscaling = autoscaling,\n",
    "    resources = resources,\n",
    "    wait = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'components': [{'build_id': 453155, 'ratio': 1, 'status': 'running'}], 'creator_request': {'enable_prediction_authz': False, 'name': 'production'}, 'date_created': '2022-11-29T20:51:47.000Z', 'date_updated': '2022-11-29T20:57:09.000Z', 'status': 'active', 'stage_id': 362951}\n"
     ]
    }
   ],
   "source": [
    "assert status['status'] == \"active\"\n",
    "file = '000001_0.jpg'\n",
    "\n",
    "with open(f\"images/{file}\", 'rb') as img:\n",
    "    img_bytes = base64.b64encode(img.read())\n",
    "    img_str = img_bytes.decode('utf-8')\n",
    "    img_str = json.dumps(img_str)\n",
    "    img_str = np.array(img_str).tolist()\n",
    "\n",
    "endpoint.get_deployed_model().predict([file, img_str])\n",
    "print(status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bb62b3221a3e8eb4d56ef3abf33b59c1166039d4d316f70559cb86bddd6b450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
