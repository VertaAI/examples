{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying an NLTK + scikit-learn (spam detection) model on Verta\n",
    "\n",
    "Within Verta, a \"Model\" can be any arbitrary function: a traditional ML model (e.g., sklearn, PyTorch, TF, etc); a function (e.g., squaring a number, making a DB function etc.); or a mixture of the above (e.g., pre-processing code, a DB call, and then a model application.) See more [here](https://docs.verta.ai/verta/registry/concepts).\n",
    "\n",
    "This notebook provides an example of how to deploy a spam detection model built using NLTK and scikit-learn on Verta as a Verta Standard Model by extending [VertaModelBase](https://verta.readthedocs.io/en/master/_autogen/verta.registry.VertaModelBase.html?highlight=VertaModelBase#verta.registry.VertaModelBase).\n",
    "\n",
    "Updated for Verta version: 0.18.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example features:\n",
    "- word similarity detection using [WordNet](https://github.com/nltk/wordnet) from **NLTK**\n",
    "- [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) vectorization using **scikit-learn**\n",
    "- **verta**'s Python client logging a `class` as a model to be instantiated at deployment time\n",
    "- predictions against a deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/VertaAI/examples/blob/main/deployment/nltk-sklearn/spam-detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "\n",
    "import wget\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Verta import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart your notebook if prompted on Colab\n",
    "try:\n",
    "    import verta\n",
    "except ImportError:\n",
    "    !python -m pip install verta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['VERTA_EMAIL'] = \n",
    "# os.environ['VERTA_DEV_KEY'] = \n",
    "# os.environ['VERTA_HOST'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(os.environ['VERTA_HOST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Log model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_url = \"http://s3.amazonaws.com/verta-starter/spam.csv\"\n",
    "train_data_filename = wget.detect_filename(train_data_url)\n",
    "if not os.path.isfile(train_data_filename):\n",
    "    wget.download(train_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(train_data_filename, delimiter=',', encoding='latin-1')\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn spam/ham to 0/1, and remove unnecessary columns\n",
    "raw_data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\n",
    "raw_data.v1 = LabelEncoder().fit_transform(raw_data.v1)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize text\n",
    "total_stopwords = set([word.replace(\"'\",'') for word in stopwords.words('english')])\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\",'')\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)\n",
    "    words = text.split()\n",
    "    words = [lemma.lemmatize(word) for word in words if (word not in total_stopwords) and (len(word)>1)] # Remove stop words\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "raw_data.v2 = raw_data.v2.apply(preprocess_text)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    raw_data.v2,\n",
    "    raw_data.v1,\n",
    "    test_size=0.15,\n",
    "    stratify=raw_data.v1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = client.set_project(\"Spam Detection\")\n",
    "expt = client.set_experiment(\"tf–idf\")\n",
    "run = client.set_experiment_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(x_train)\n",
    "\n",
    "x_train_vec = vectorizer.transform(x_train).toarray()\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vec = vectorizer.transform(x_test).toarray()\n",
    "y_pred = model.predict(x_test_vec)\n",
    "\n",
    "m_confusion_test = confusion_matrix(y_test, y_pred)\n",
    "display(pd.DataFrame(data=m_confusion_test,\n",
    "                     columns=['Predicted 0', 'Predicted 1'],\n",
    "                     index=['Actual 0', 'Actual 1']))\n",
    "\n",
    "print(\"This model misclassifies {} genuine SMS as spam\"\n",
    "      \" and misses only {} SPAM.\".format(m_confusion_test[0,1], m_confusion_test[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "run.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Log model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This example logs a `class` (instead of an object instance) as a model.\n",
    "This allows for custom setup configuration in the class's `__init__()` method,  \n",
    "and access to logged artifacts at deployment time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and upload weights\n",
    "model_param = {}\n",
    "model_param['coef'] = model.coef_.reshape(-1).tolist()\n",
    "model_param['intercept'] = model.intercept_.tolist()\n",
    "\n",
    "json.dump(model_param, open(\"weights.json\", \"w\"))\n",
    "\n",
    "run.log_artifact(\"weights\", open(\"weights.json\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize and upload vectorizer\n",
    "run.log_artifact(\"vectorizer\", vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Define model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model—with its pre-trained weights and serialized vectorizer—will require some setup at deployment time.\n",
    "\n",
    "To support this, the Verta platform allows a model to be defined as a `class` that will be instantiated when it's deployed.  \n",
    "This class should have provide the following interface:\n",
    "\n",
    "- `__init__(self, artifacts)` where `artifacts` is a mapping of artifact keys to filepaths. This will be explained below, but Verta will provide this so you can open these artifact files and set up your model. Other initialization steps would be in this method, as well.\n",
    "- `predict(self, data)` where `data`—like in other custom Verta models—is a list of input values for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamModel():    \n",
    "    def __init__(self, artifacts):\n",
    "        from nltk.corpus import stopwords  # needs to be re-imported to remove local file link\n",
    "        \n",
    "        # get artifact filepaths from `artifacts` mapping\n",
    "        weights_filepath = artifacts['weights']\n",
    "        vectorizer_filepath = artifacts['vectorizer']\n",
    "\n",
    "        # load artifacts\n",
    "        self.weights = json.load(open(weights_filepath, \"r\"))\n",
    "        self.vectorizer = pickle.load(open(vectorizer_filepath, \"rb\"))\n",
    "        \n",
    "        # reconstitute logistic regression\n",
    "        self.coef_ = np.array(self.weights[\"coef\"])\n",
    "        self.intercept_ = self.weights[\"intercept\"]\n",
    "        \n",
    "        # configure text preprocessing\n",
    "        self.total_stopwords = set([word.replace(\"'\",'') for word in stopwords.words('english')])\n",
    "        self.lemma = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"'\",'')\n",
    "        text = re.sub('[^a-zA-Z]',' ',text)\n",
    "        words = text.split()\n",
    "        words = [self.lemma.lemmatize(word) for word in words if (word not in self.total_stopwords) and (len(word)>1)] # Remove stop words\n",
    "        text = \" \".join(words)\n",
    "        return text     \n",
    "        \n",
    "    def predict(self, data):\n",
    "        predictions = []\n",
    "        for inp in data:\n",
    "            # preprocess input\n",
    "            processed_text = self.preprocess_text(inp)\n",
    "            inp_vec = self.vectorizer.transform([inp]).toarray()\n",
    "            \n",
    "            # make prediction\n",
    "            prediction = (np.dot(inp_vec.reshape(-1), self.coef_.reshape(-1)) + self.intercept_)[0]\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def example(self):\n",
    "        return [\"FREE FREE FREE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we logged artifacts with the keys `\"weights\"` and `\"vectorizer\"`.  \n",
    "You can obtain an `artifacts` mapping mentioned above using `run.fetch_artifacts(keys)` to work with locally.  \n",
    "A similar mapping—that works identically—will be passed into `__init__()` when the model is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = run.fetch_artifacts([\"weights\", \"vectorizer\"])\n",
    "\n",
    "spam_model = SpamModel(artifacts=artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spam_model.example()\n",
    "prediction = spam_model.predict(data)\n",
    "\n",
    "print(data, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Log model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.log_model(\n",
    "    model=SpamModel,\n",
    "    model_api=ModelAPI(data, prediction),\n",
    "    artifacts=['weights', 'vectorizer'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to make sure we provide every package involved in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.environment import Python\n",
    "\n",
    "run.log_environment(Python([\n",
    "    \"nltk\",\n",
    "    \"numpy\",\n",
    "    \"sklearn\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need to ensure that the appropriate NLTK packages are available during deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_setup_script(\"\"\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy model to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = client.get_or_create_endpoint(\"spam-detection\")\n",
    "endpoint.update(run, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.get_deployed_model().predict(spam_model.example())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
