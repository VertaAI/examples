{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "{    \"tags\": [\n",
    "        \"hide-output\",\n",
    "        \"hide-input\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "VERTA_DOCKER=0\n",
    "SPARK_UDF=1\n",
    "PG_UDF=2\n",
    "\n",
    "def export_as(self, pkg_format=VERTA_DOCKER, **kwargs):\n",
    "    print(kwargs)\n",
    "    if pkg_format == SPARK_UDF:\n",
    "        # Create a spark UDF > load UDF into SparkSession initiatied above. UDF named \"xsell_udf\"\n",
    "        import mlflow.sklearn\n",
    "        import mlflow.pyfunc\n",
    "        import secrets\n",
    "        random_dir = secrets.token_hex(4)\n",
    "        # Construct and save the model\n",
    "        model_path = \"/opt/models/\" + random_dir\n",
    "        mlflow.sklearn.save_model(self.get_model(), model_path)\n",
    "        # Create udf based on saved model location and load into cluster. UDF named xsell_udf\n",
    "        spark_udf = mlflow.pyfunc.spark_udf(kwargs[\"spark_session\"], model_path)\n",
    "        return spark_udf\n",
    "    elif pkg_format == VERTA_DOCKER:\n",
    "        pass\n",
    "    elif pkg_format == PG_UDF:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.registry._entities import RegisteredModelVersion\n",
    "\n",
    "RegisteredModelVersion.export_as = export_as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056834,
     "end_time": "2020-11-28T23:01:34.209435",
     "exception": false,
     "start_time": "2020-11-28T23:01:34.152601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1><center>Health Insurance Cross Sell Prediction</center></h1>\n",
    "We are an Insurance company that provides Health Insurance to our customers. We need to build a model to predict whether the policyholders (customers) from last year will also be interested in Vehicle Insurance that we provide and sell too. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if needed\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate spark session, code subject to change based on your spark env\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3.9'\n",
    "conf = SparkConf().setAppName(\"VertaSpark\").setMaster(\"spark://ip-172-31-8-213.us-west-2.compute.internal:7077\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.getConf().getAll()\n",
    "sess = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Verta setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import verta, os\n",
    "HOST = \"XXXXX.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"Insurance Life2Auto Cross-sell\"\n",
    "EXPERIMENT_NAME = \"Logistic Regression\"\n",
    "WORKSPACE = \"XXXXX\"\n",
    "os.environ['VERTA_EMAIL'] = \"XXXXXX\"\n",
    "os.environ['VERTA_DEV_KEY'] = \"XXXXXXX\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate client and libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "got existing Project: Insurance Life2Auto Cross-sell\n",
      "got existing Experiment: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME, workspace=WORKSPACE)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model = client.get_registered_model(name=\"Life2AutoCrossSell\", workspace=WORKSPACE)\n",
    "model_version=registered_model.get_version(name=\"v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spark_session': <pyspark.sql.session.SparkSession object at 0x7f29b8265a90>}\n"
     ]
    }
   ],
   "source": [
    "model_as_spark_udf = model_version.export_as(pkg_format=SPARK_UDF, spark_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab some test data\n",
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "test_data_url = \"https://verta-demo.s3-us-west-2.amazonaws.com/xselltest.csv\"\n",
    "test_data_filename = wget.detect_filename(test_data_url)\n",
    "if not os.path.isfile(test_data_filename):\n",
    "    wget.download(test_data_url)\n",
    "test = pd.read_csv(test_data_filename)\n",
    "test = test.drop(['id'], axis=1)\n",
    "\n",
    "test.loc[test['Gender'] == 'Male', 'Gender'] = 1\n",
    "test.loc[test['Gender'] == 'Female', 'Gender'] = 0\n",
    "\n",
    "test.loc[test['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\n",
    "test.loc[test['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\n",
    "test.loc[test['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\n",
    "\n",
    "test.loc[test['Vehicle_Damage'] == 'Yes', 'Vehicle_Damage'] = 1\n",
    "test.loc[test['Vehicle_Damage'] == 'No', 'Vehicle_Damage'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run UDF on dataset, display as df\n",
    "cols = test.columns\n",
    "dfs = sess.createDataFrame(test)\n",
    "dfs = dfs.withColumn('prediction', model_as_spark_udf(*cols.values))\n",
    "pandas_df = dfs.toPandas()\n",
    "pandas_df.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "papermill": {
   "duration": 2018.36416,
   "end_time": "2020-11-28T23:35:07.745080",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-28T23:01:29.380920",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
