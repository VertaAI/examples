{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering a TF/Keras (EfficientNet ImageNet Classification) model on Verta\n",
    "\n",
    "Within Verta, a \"Model\" can be any arbitrary function: a traditional ML model (e.g., sklearn, PyTorch, TF, etc); a function (e.g., squaring a number, making a DB function etc.); or a mixture of the above (e.g., pre-processing code, a DB call, and then a model application.) See more [here](https://docs.verta.ai/verta/registry/concepts).\n",
    "\n",
    "This notebook provides an example of how to catalog a Keras model on Verta as a Verta Standard Model by convinience functions.\n",
    "\n",
    "Updated for Verta version: 0.21.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through initializing EfficientNet Classification model through keras applications, and cataloging them to the Verta platform."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/VertaAI/examples/blob/registry_examples/registry/tensorflow/keras-imagenet-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install verta\n",
    "!python -m pip install tensorflow\n",
    "!python -m pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Register model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model has to exist before we can register, so we will instantiate one here in our notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Info**\n",
    "\n",
    "EfficientNetV2 are a family of image classification models, which achieve better parameter efficiency and faster training speed than prior arts. Built upon EfficientNetV1, EfficientNetV2 models use neural architecture search (NAS) to jointly optimize model size and training speed, and are scaled up in a way for faster training and inference speed.\n",
    "\n",
    "Reference Paper: [EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298) Git Repo: [EfficientNetV2 TensorFlow Repo](https://github.com/google/automl/tree/master/efficientnetv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.EfficientNetV2M(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\",\n",
    "    include_preprocessing=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can validate that our model can produce predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "# Step 1: Load Image\n",
    "img_path = \"n02106662_German_shepherd.JPEG\"\n",
    "if not os.path.exists(img_path):\n",
    "    wget.download(\"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n02106662_German_shepherd.JPEG\")\n",
    "img = tf.keras.preprocessing.image.load_img(img_path, target_size=(480, 480))\n",
    "\n",
    "# Step 2: Preprocess the image\n",
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Step 3: Use the model and print the predicted category\n",
    "print(tf.keras.applications.efficientnet_v2.decode_predictions(model.predict(x, verbose=0), top=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Register Model to Verta Model Catalog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is in a good shape, we can register it into the Verta platform.\n",
    "\n",
    "We'll connect to Verta through the [Verta Python Client](https://verta.readthedocs.io/en/main/_autogen/verta.Client.html)\n",
    "create a [registered model](https://verta.readthedocs.io/en/master/_autogen/verta.registry.entities.RegisteredModel.html) for our EfficientNet ImageNet Classification Model  \n",
    "and a [version](https://verta.readthedocs.io/en/master/_autogen/verta.registry.entities.RegisteredModelVersion.html) to associate this particular model with.\n",
    "\n",
    "All of these can be viewed in the Verta web app once they are created.\n",
    "\n",
    "Note: If your model uses CustomObject you have to register the model into the catalog using a serialized version of the model by extending the [VertaModelBase](https://verta.readthedocs.io/en/master/_autogen/verta.registry.VertaModelBase.html?highlight=VertaModelBase#verta.registry.VertaModelBase) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your credentials in this cell or anywhere above this along with the code snippet to connect to Verta Platform\n",
    "\n",
    "from verta import Client\n",
    "\n",
    "client = Client(\n",
    "        #   host=\"app.verta.ai\",\n",
    "        #   email=\"user@verta.ai\",\n",
    "        #   dev_key=\"a765b2de-786d-466c-b2d8-thiye06f80d5\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Get a Verta registered model\n",
    "\n",
    "registered_model = client.get_or_create_registered_model(\n",
    "    \"EfficientNet\",\n",
    "    desc=\"Family of EfficientNet models for ImageNet Classification - 1000 Classes\",\n",
    "    labels=[\"CV\", \"Classification\", \"Neural Net\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verta.environment import Python\n",
    "\n",
    "# uncommment the below if you want to load it from a saved model (h5 or tf saved)\n",
    "# import tf\n",
    "# model = tf.keras.models.load_model(\"<file_to_model.h5> or <path_to_tf_saved_model>\")\n",
    "\n",
    "model_version_v1 = registered_model.create_standard_model_from_keras(\n",
    "    model, # The loaded model object, can be the one trained in the same file or loaded from keras load_model function\n",
    "    environment=Python(requirements=[ # Add the required libraries for the model to run\n",
    "    \"tensorflow\"\n",
    "    ]), \n",
    "    name=\"V2M-tf\", # Name to identify the version in the model versions tab\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And That's it. You should now be able to see your Model, on your Catalog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
